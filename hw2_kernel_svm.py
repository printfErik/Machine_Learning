# -*- coding: utf-8 -*-
"""kernel_svm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uVe7XgxfeJXpmdtQfJih5-WW4NbdX75M
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cvxopt
cvxopt.solvers.options['show_progress'] = False

# training data set
def rbf_svm_train(X, y, c, sigma):
  print(np.shape(X))
  size = np.shape(X)[0]

  # define P,q,G,h for corresponding parameter in qp solver
  P = np.zeros((size,size))
  for i in range(size):
    for j in range(size):
      P[i,j] = y[i]*y[j] * np.exp(-1*(np.linalg.norm(X[i]-X[j]))**2/(2*sigma**2))
  P = cvxopt.matrix(P)
  q = -cvxopt.matrix(np.ones(size))
  G = cvxopt.matrix(np.concatenate((np.eye(size),-np.eye(size))))
  h = cvxopt.matrix(np.concatenate((np.ones(size)*c,np.zeros(size))))
  ans = cvxopt.solvers.qp(P,q,G,h)
  alpha = ans['x']
  return alpha

# using computed weight to predict
def rbf_svm_predict(test_X,train_X,train_y,alpha,sigma):
  size = np.shape(test_X)[0]
  ans = np.zeros(size)
  for i in range(size):
    for j in range(np.shape(train_X)[0]):
      ans[i] = ans[i] + np.exp(-1* (np.linalg.norm(test_X[i] - train_X[j]))**2/(2*sigma**2))*alpha[j]*train_y[j]
  return np.sign(ans)

 


def rbf_k_fold_cv(train_data, test_data, k, c, sigma):
  X_shuffled = train_data[:,:2]
  y_shuffled = train_data[:,2]

  X_test = test_data[:,:2]
  y_test = test_data[:,2]

  X_train, y_train, X_valid, y_valid = get_next_train_valid(X_shuffled, y_shuffled, k)

  # train model
  alpha = rbf_svm_train(X_train,y_train,c,sigma)
  #print(np.shape(alpha))
  # model accuracy for trainning set
  train_label = rbf_svm_predict(X_train,X_train,y_train,alpha,sigma)
  train_label = np.reshape(train_label,len(X_train))
  train_accuracy = np.sum(train_label == y_train)/len(X_train)
 
  # model accuracy for validation set
  cv_label = rbf_svm_predict(X_valid,X_train,y_train,alpha,sigma)
  cv_label = np.reshape(cv_label,len(X_valid))
  cv_accuracy = np.sum(cv_label == y_valid)/len(X_valid)

  # model accuracy for test set
  test_label = rbf_svm_predict(X_test,X_train,y_train,alpha,sigma)
  test_label = np.reshape(test_label,len(X_test))
  test_accuracy = np.sum(test_label == y_test)/len(X_test)


  return train_accuracy, cv_accuracy, test_accuracy

def get_next_train_valid(X_shuffled, y_shuffled, itr):
  size = X_shuffled.shape[0]

  # select the correct slice of valid data set with each fold has equal length 29
  X_valid = X_shuffled[itr * (size//10) : (itr+1)*(size//10),:]
  y_valid = y_shuffled[itr * (size//10) : (itr+1)*(size//10)]

  # delete valid set to get training set
  X_train = np.delete(X_shuffled, np.s_[itr * (size//10) : (itr+1) * (size//10) : 1], 0)
  y_train = np.delete(y_shuffled, np.s_[itr * (size//10) : (itr+1) * (size//10) : 1], 0)
  return X_train, y_train, X_valid, y_valid



samples = pd.read_csv("hw2data.csv",header = None)
#print(np.shape(samples))
data = np.array(samples)
np.random.shuffle(data)
#print(np.shape(data))

# seperate test and cross-validation data
test_data = data[:np.shape(data)[0]//5]
cross_data = data[np.shape(data)[0]//5:]

# hyperparameters 
C =  [0.001, 0.01]
logC = [-3,-2]

sigma = [0.1, 1, 10]
logSigma = [-1,0,1]

train_acc = np.zeros((len(C),len(sigma)))
cv_acc = np.zeros((len(C),len(sigma)))
test_acc = np.zeros((len(C),len(sigma)))

#compute accuracy for trainning, validation and test set
for i in range(len(C)):
  for j in range(len(sigma)):
    for k in range(10):
      train_accuracy, cv_accuracy, test_accuracy = rbf_k_fold_cv(cross_data,test_data,k,C[i],sigma[j])
      train_acc[i][j] += train_accuracy
      cv_acc[i][j] += cv_accuracy
      test_acc[i][j] += test_accuracy
    train_acc[i][j] = train_acc[i][j]/10
    print(train_acc[i][j])
    cv_acc[i][j] = cv_acc[i][j]/10
    print(cv_acc[i][j])
    test_acc[i][j] = test_acc[i][j]/10
    print(test_acc[i][j])



# draw heat map 
fig, ax = plt.subplots()
im = ax.imshow(test_acc)

ax.set_xticks(np.arange(len(logSigma)))
ax.set_yticks(np.arange(len(logC)))
ax.set_xticklabels(logSigma)
ax.set_yticklabels(logC)
ax.set_ylabel('log(C)')
ax.set_xlabel('log(sigma)')
for i in range(len(logSigma)):
    for j in range(len(logC)):
        text = ax.text(i, j, test_acc[j, i],ha="center", va="center", color="w")

ax.set_title("Heat map for various C and sigma")
fig.tight_layout()
plt.show()


# after getting optimal C and sigma, using them to plot accuracy for each fold
optimal_acc_cv = np.zeros(10)
optimal_acc_test = np.zeros(10)
for k in range(10):
  train_accuracy, cv_accuracy, test_accuracy = rbf_k_fold_cv(cross_data,test_data,k,0.01,0.1)
  optimal_acc_cv[k] = cv_accuracy
  print(cv_accuracy)
  optimal_acc_test[k] = test_accuracy
  print(test_accuracy)

x_axis = np.arange(10)  
width = 0.35 
fig2, ax2 = plt.subplots()
rects1 = ax2.bar(x_axis - width/2, optimal_acc_cv, width, label='Validation')
rects3 = ax2.bar(x_axis + width/2, optimal_acc_test, width, label='Test')


ax2.set_ylabel('Accuracy')
ax2.set_xlabel('k Fold')
ax2.set_title('Accuracy by Folds and Data Set')
ax2.set_xticks(x_axis)
ax2.set_xticklabels(('1','2','3','4','5','6','7','8','9','10'))
ax2.legend()

fig2.tight_layout()

plt.show()